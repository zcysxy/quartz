{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c8769679",
   "metadata": {},
   "source": [
    "---\n",
    "{\"publish\":true,\"title\":\"Tail Bounds and Concentration Inequalities\",\"created\":\"2025-05-26T19:28:12\",\"modified\":\"2025-06-07T09:16:59\",\"cssclasses\":\"\",\"state\":\"done\",\"sup\":[\"[[Statistics]]\"],\"alias\":null,\"type\":\"jupyter\",\"related\":[\"[[Concentration Inequality]]\"]}\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023969a6",
   "metadata": {},
   "source": [
    "\n",
    "# Tail Bounds and Concentration Inequalities\n",
    "\n",
    "Both tail bounds and concentration inequalities describe the probability that a random variable deviates from some certain value, such as its mean.\n",
    "In this notebook, we will explore how tail bounds and concentration inequalities predict the behavior of random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting aesthetics\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f8aa0",
   "metadata": {},
   "source": [
    "## Gaussian Tail Bound\n",
    "\n",
    "The tail bound of a Gaussian random variable $X \\sim \\mathcal{N}(0,1)$ is described by [[Concentration Inequality#Mill's Ratio\\|Mill's ratio]]:\n",
    "$$\n",
    "P(X \\geq x) \\le \\frac{e^{-x^2 /2}}{x\\sqrt{2\\pi}} \n",
    "$$\n",
    "We plot the theoretical tail bound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed138e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Gaussian Tail Bound using Mill's Ratio\n",
    "\n",
    "def mills_ratio(x):\n",
    "    \"\"\"Mill's ratio approximation of the upper tail probability for N(0,1)\"\"\"\n",
    "    return (np.exp(-x**2 / 2) / np.sqrt(2 * np.pi)) / x\n",
    "\n",
    "# Domain for x\n",
    "x_vals = np.linspace(2, 6, 40)\n",
    "tail_gauss_theo = mills_ratio(x_vals)\n",
    "\n",
    "# Plot\n",
    "fig_gaussian, ax = plt.subplots()\n",
    "ax.plot(x_vals, tail_gauss_theo, label=\"Mill's ratio\", color='darkorange')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('P(X ≥ x)')\n",
    "ax.set_title('Gaussian Tail Bound')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e90abb",
   "metadata": {},
   "source": [
    "We now demonstrate the tail bound of a Gaussian random variable using simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_tail_probabilities(dist_sampler, x_vals, n_samples=int(1e7)):\n",
    "    \"\"\"\n",
    "    Simulate tail probabilities P(X ≥ x) for a given sampler.\n",
    "    \n",
    "    Parameters:\n",
    "    - dist_sampler: function that returns samples from the target distribution\n",
    "    - x_vals: array of threshold values\n",
    "    - n_samples: number of samples to draw\n",
    "    \n",
    "    Returns:\n",
    "    - A list of estimated tail probabilities for each x in x_vals\n",
    "    \"\"\"\n",
    "    samples = dist_sampler(n_samples)\n",
    "    return [np.mean(samples >= x) for x in x_vals]\n",
    "\n",
    "# Simulate Gaussian tail probabilities\n",
    "tail_gauss_emp = simulate_tail_probabilities(np.random.randn, x_vals)\n",
    "\n",
    "# Plot\n",
    "ax.plot(x_vals, tail_gauss_emp, label=\"Simulation\", linestyle='--', marker='o', color='blue')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240501e3",
   "metadata": {},
   "source": [
    "## t-Distribution Tail Bound\n",
    "\n",
    "Gaussian distribution is known to have a *light* tail bound, i.e., an exponential decay in the tail probability. [[t Distribution]], on the other hand, is known to have a *heavy* tail bound.\n",
    "For example, $t_{1}$, or [[Cauchy Distribution\\|Cauchy]], distribution, has a tail bound of\n",
    "$$\n",
    "P(X \\geq x) = \\frac{1}{\\pi} \\arctan x^{-1} \\asymp \\frac{1}{\\pi x}.\n",
    "$$\n",
    "\n",
    "We plot the theoretical and empirical Cauchy tail bound against the theoretical Gaussian tail bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc01762",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_cauchy_theo = np.arctan(1 / x_vals) / np.pi\n",
    "tail_cauchy_emp = simulate_tail_probabilities(lambda n: np.random.standard_cauchy(n), x_vals, n_samples=int(1e2))\n",
    "\n",
    "fig_cauchy, ax = plt.subplots()\n",
    "ax.plot(x_vals, tail_gauss_theo, label=\"Gaussian Tail Bound\", color='darkorange')\n",
    "ax.plot(x_vals, tail_cauchy_theo, label=\"Cauchy Tail Bound\", color='green')\n",
    "ax.plot(x_vals, tail_cauchy_emp, label=\"Cauchy Simulation\", linestyle='--', marker='o', color='blue')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('P(X ≥ x)')\n",
    "ax.set_title('Cauchy Tail Bound vs Gaussian Tail Bound')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1190ce",
   "metadata": {},
   "source": [
    "## Chebyshev Inequality\n",
    "\n",
    "Different from the tail bound of a specific random variable, concentration inequalities provide bounds for a class of random variables.\n",
    "\n",
    "For example, for any random variable $X$ with finite mean and variance, Chebyshev's inequality gives\n",
    "$$\n",
    "P(|X - \\mathbb{E}[X]| \\geq t) \\leq \\frac{\\mathrm{Var}(X)}{t^2}.\n",
    "$$\n",
    "\n",
    "Because of the generality, it's often too loose for certain distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91aa68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_gauss_chebyshev = np.var(np.random.randn(int(1e7))) / (x_vals**2)\n",
    "\n",
    "fig_concen, ax = plt.subplots()\n",
    "ax.plot(x_vals, tail_gauss_theo, label=\"Gaussian Tail Bound\", color='darkorange')\n",
    "ax.plot(x_vals, tail_gauss_chebyshev, label=\"Chebyshev's Inequality\", color='purple')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('P(X ≥ t)')\n",
    "ax.set_title(\"Gaussian Tail Bound vs Concentration Inequalities\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c77fff",
   "metadata": {},
   "source": [
    "## Hoeffding's Inequality\n",
    "\n",
    "For bounded iid random variables $X_{i}\\in [0,1]$, [[Hoeffding's Inequality]] states that\n",
    "$$\n",
    "P\\left( \\left| \\frac{1}{n}\\sum_{i=1}^{n}X_{i} - \\mathbb{E}[X] \\right| \\geq t \\right) \\leq 2e^{-2nt^2}.\n",
    "$$\n",
    "For a single standard Gaussian random variable, Hoeffding's inequality reduces to the Chernoff bound:\n",
    "$$\n",
    "P(X \\geq t) \\leq e^{-t^2 / 2},\n",
    "$$\n",
    "which is close to the exact Gaussian tail bound, with a slight difference caused by constants and the scaling of $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5906133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_gauss_hoeffding = np.exp(-(x_vals**2)/2)\n",
    "\n",
    "ax.plot(x_vals, tail_gauss_hoeffding, label=\"Hoeffding's Inequality\", color='green')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411ec23",
   "metadata": {},
   "source": [
    "We now explore different concentration inequalities for non-Gaussian distributions. We use iid uniform random variables as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sample_means(n_trials, n_vars=10):\n",
    "    \"\"\"\n",
    "    Simulate sample means of n_vars iid Uniform[0,1] variables over n_trials.\n",
    "    \"\"\"\n",
    "    samples = np.random.uniform(0, 1, size=(n_trials, n_vars))\n",
    "    return samples.mean(axis=1)\n",
    "\n",
    "# Parameters\n",
    "n_trials = int(1e5)\n",
    "sample_means = simulate_sample_means(n_trials)\n",
    "mean_estimate = 0.5\n",
    "var_uniform = 1/12\n",
    "var_sample_mean = var_uniform / 10\n",
    "\n",
    "x_vals = np.linspace(0.05, 0.5, 40)\n",
    "\n",
    "tail_mean_emp = np.array([np.mean(np.abs(sample_means - mean_estimate) >= t) for t in x_vals])\n",
    "tail_mean_chebyshev = var_sample_mean / (x_vals**2)\n",
    "tail_mean_hoeffding = 2 * np.exp(-2 * 10 * (x_vals**2))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_vals, tail_mean_emp, label=\"Empirical Sample Mean\", linestyle='--', marker='o', color='blue')\n",
    "plt.plot(x_vals, tail_mean_chebyshev, label=\"Chebyshev's Inequality\", color='purple')\n",
    "plt.plot(x_vals, tail_mean_hoeffding, label=\"Hoeffding's Inequality\", color='green')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(r'$P(|\\bar{X} - \\mathbb{E}[\\bar{X}]| \\geq t)$')\n",
    "plt.title('Concentration Inequalities for Sample Mean of Uniform Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5cefd",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "- Tail bounds describe the probability of a random variable deviating from a specific value, such as its mean.\n",
    "- Different distributions have different tail behaviors, with Gaussian having a light tail and Cauchy having a heavy tail. Tight-tailed distributions are easier to control and predict.\n",
    "- Different from exact tail bounds, concentration inequalities provide general bounds for a class of random variables, such as Chebyshev's inequality and Hoeffding's inequality.\n",
    "- Compared to exact tail bounds, concentration inequalities are often too *conservative* and thus provide looser bounds."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
