{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6143179a",
   "metadata": {},
   "source": [
    "---\n",
    "{\"publish\":true,\"title\":\"Best Estimator for Uniform Distribution Parameter\",\"created\":\"2022-12-06T00:26:27\",\"modified\":\"2025-06-12T03:05:30\",\"cssclasses\":\"\",\"aliases\":null,\"type\":\"jupyter\",\"sup\":[\"[[Probability Theory]]\",\"[[Uniform Distribution]]\"],\"state\":\"done\",\"related\":[\"[[Maximum Likelihood Estimation]]\"],\"reference\":[\"https://math.unm.edu/~knrumsey/pdfs/projects/Uniform.pdf\"]}\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20dc3d",
   "metadata": {},
   "source": [
    "\n",
    "# Best Estimator for Uniform Distribution Parameter\n",
    "\n",
    "We want to estimate the parameter $\\theta$ of a [[Uniform Distribution]] given $n$ i.i.d samples $X_i \\overset{ \\text{i.i.d.} }{ \\sim } \\operatorname{Unif}[0,\\theta ]$.\n",
    "\n",
    "- [?] So, what is the *best estimator*?\n",
    "\n",
    "First, we need to define the metric. We use [[Mean Squared Error]]. We know for an estimator $\\hat{\\theta}$ of $\\theta$, its [[Mean Squared Error\\|MSE]] is\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta},\\theta ) = \\operatorname{Bias}(\\hat{\\theta})^{2} + \\operatorname{SE}(\\hat{\\theta})^{2},\n",
    "$$\n",
    "where $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$ is the bias of the estimator, and $\\operatorname{SE}(\\hat{\\theta}) = \\sqrt{\\operatorname{Var}(\\hat{\\theta})}$ is the standard error of the estimator.\n",
    "\n",
    "We will also touch on [[Evaluating an Estimator#Asymptotic Normality]] for certain estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d182c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-colorblind')\n",
    "# sns.set_palette(\"husl\")\n",
    "\n",
    "# Set parameters for demonstrations\n",
    "np.random.seed(42)\n",
    "theta_true = 1.0  # True parameter value\n",
    "sample_sizes = [5, 10, 20, 50, 100]\n",
    "num_simulations = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f347e",
   "metadata": {},
   "source": [
    "## First Attempts\n",
    "\n",
    "Note that the [[Expectation]] of $\\operatorname{Unif}[0,\\theta]$ is $\\theta /2$, which gives $\\theta = 2\\mathbb{E}[X]$. Therefore, the very first estimator we can think of is to replace the expectation with a sample value:\n",
    "$$\n",
    "\\hat{\\theta}^{(1)} = 2 X_{1}.\n",
    "$$\n",
    "\n",
    "We know this estimator is unbiased and thus its MSE is\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(1)}) = \\Var(2X_{1}) = 4 \\Var(X_{1}) = 4\\cdot \\frac{\\theta^{2}}{12} = \\frac{\\theta^{2}}{3},\n",
    "$$\n",
    "which is a constant regardless of the sample size $n$.\n",
    "\n",
    "Obviously, $\\hat{\\theta}^{(1)}$ is not satisfactory as it only uses the information of one sample. More generally, we can consider an estimator that uses $k \\le n$ samples:\n",
    "$$\n",
    "\\hat{\\theta}^{(k)} = \\frac{1}{k} \\sum_{i=1}^{k} 2X_{i},\n",
    "$$\n",
    "which is still unbiased but has a reduced standard error:\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(k)}) = \\Var\\left( \\frac{1}{k} \\sum_{i=1}^{k}2X_{i} \\right)  = \\frac{4}{k^{2}} \\sum_{i=1}^{k}\\Var(X_{i}) = \\frac{\\theta^{2}}{3k}.\n",
    "$$\n",
    "Again, the MSE is a constant w.r.t $n$.\n",
    "\n",
    "We plot the histogram of $\\hat{\\theta}^{(k)}$ for different $k$ values to see how the distribution changes with sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_sample_estimator(sample,k):\n",
    "    assert k <= len(sample), \"k must be less than or equal to the sample size\"\n",
    "    return 2 * np.mean(sample[:k], axis=0)\n",
    "\n",
    "n_sim = sample_sizes[2]\n",
    "samples_hist = np.random.uniform(0, theta_true, size=(n_sim, num_simulations)) # reuse for later simulations\n",
    "ks = list(range(0,n_sim,5))\n",
    "ks[0] = 1\n",
    "estmates = np.empty((len(ks), num_simulations))\n",
    "\n",
    "for kind in range(len(ks)):\n",
    "    estmates[kind, :] = k_sample_estimator(samples_hist, ks[kind])\n",
    "\n",
    "# Plot all histograms together in a plot\n",
    "plt.subplot(1, 1, 1)\n",
    "for kind in range(len(ks)):\n",
    "    sns.histplot(estmates[kind, :], stat='density', label=f'$k={ks[kind]}$', bins=20, binrange=(0,2), alpha=0.5)\n",
    "plt.axvline(theta_true, color='red', linestyle='--', label='True $\\theta$')\n",
    "plt.title(f'Histograms of $\\\\hat{{\\\\theta}}^{{(k)}}$ for different k (n={n_sim})')\n",
    "plt.xlabel('$\\\\hat{\\\\theta}^{(k)}$')\n",
    "plt.yticks([])\n",
    "plt.ylabel('')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057cbae",
   "metadata": {},
   "source": [
    "## Method of Moments\n",
    "\n",
    "A natural extension is to use all $n$ samples:\n",
    "$$\n",
    "\\hat{\\theta }^{(\\mathrm{MM})} = \\frac{1}{n}\\sum_{i=1}^{n}2X_{i} = 2\\overline{X}.\n",
    "$$\n",
    "Since the above is equivalent to solving the estimation equation\n",
    "$$\n",
    "2\\overline{X} = \\hat{\\mathbb{E}}_{n}2X = \\mathbb{E}_{\\hat{\\theta}^{(\\mathrm{MM})}} 2X = \\hat{\\theta}^{(\\mathrm{MM})},\n",
    "$$\n",
    "the resultant estimator is a [[Method of Moments\\|Moment Estimator]]. In other words, we plug in the sample mean as the true mean (first moment) to get the estimation.\n",
    "The mean squared error is the same as the previous case with $n$ samples:\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MM})}) = \\frac{\\theta^{2}}{3n}.\n",
    "$$\n",
    "\n",
    "We compare the MSE and histogram of the moment estimator with $\\hat{\\theta}^{(2)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moment_estimator(sample):\n",
    "\t\treturn 2 * np.mean(sample, axis=0)\n",
    "\n",
    "estimators_mse = {\n",
    "    'two samples': lambda n: theta_true**2 / (3 * 2),\n",
    "    'MM': lambda n: theta_true**2 / (3 * n),\n",
    "}\n",
    "\n",
    "# Plot MSE vs sample size n\n",
    "ax_mse = plt.subplot(2, 1, 1)\n",
    "n_mse = range(2,30)\n",
    "def plot_mse(ax,estimators_mse,n_mse):\n",
    "    for m in estimators_mse:\n",
    "        ax.plot(n_mse, [estimators_mse[m](n) for n in n_mse], label=f'{m}')\n",
    "    ax.set_xlabel('Sample Size n')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title('MSE of Estimators vs Sample Size')\n",
    "    ax.legend()\n",
    "plot_mse(ax_mse, estimators_mse, n_mse)\n",
    "\n",
    "\n",
    "estimators = {\n",
    "    'two samples': lambda sample: k_sample_estimator(sample, 2),\n",
    "    'MM': moment_estimator,\n",
    "}\n",
    "\n",
    "estimates = np.empty((0, num_simulations)) # expand this array in later simulations\n",
    "for i, (name, estimator) in enumerate(estimators.items()):\n",
    "    # Check if estimates has already been filled for this method\n",
    "    if estimates.shape[0] > i:\n",
    "        continue\n",
    "    estimates = np.vstack((\n",
    "        estimates,\n",
    "        estimator(samples_hist)\n",
    "    ))\n",
    "\n",
    "# Plot histograms of the estimators\n",
    "ax_hist = plt.subplot(2, 1, 2)\n",
    "def plot_hist(ax, estimators, estimates):  \n",
    "    for i, (name, estimator) in enumerate(estimators.items()):\n",
    "        sns.histplot(estimates[i, :], stat='density', label=name, bins=20, binrange=(0,2), alpha=0.5)\n",
    "    ax.axvline(theta_true, color='red', linestyle='--', label='$\\\\theta$')\n",
    "    ax.set_title(f'Histograms of Estimators (n={n_sim})')\n",
    "    ax.set_xlabel('$\\\\hat{\\\\theta}$')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_yticks([])\n",
    "    ax.legend()\n",
    "plot_hist(ax_hist, estimators, estimates)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc8cc1",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MLE})}) = \\operatorname{Var}(\\hat{\\theta}^{(\\mathrm{MLE})}) + \\operatorname{Bias}(\\hat{\\theta}^{(\\mathrm{MLE})})^{2}.\n",
    "$$\n",
    "\n",
    "To get $\\operatorname{Var}(\\hat{\\theta}^{(\\mathrm{MLE})})$ and $\\operatorname{Bias}(\\hat{\\theta}^{(\\mathrm{MLE})})$, we need first get the distribution of $\\hat{\\theta}^{(\\mathrm{MLE})} = \\max_{i} X _i$.\n",
    "$$\n",
    "P(\\hat{\\theta}^{(\\mathrm{MLE})} \\le x) = P(\\max_{i} X _i \\le x) = \\prod_{i}P(X _i\\le x) = (x / \\theta)^{n}.\n",
    "$$\n",
    "\n",
    "Therefore, the [[Probability Density Function\\|PDF]] of $\\hat{\\theta}^{(\\mathrm{MLE})}$ is\n",
    "$$\n",
    "f(x) = \\frac{nx^{n-1}}{\\theta ^{n}}, \\quad x \\in [0,\\theta ]\n",
    "$$\n",
    "\n",
    "Then, we have\n",
    "$$\\tag{1}\n",
    "\\mathbb{E}[\\hat{\\theta}^{(\\mathrm{MLE})}] = \\int _{0}^{\\theta}xf(x) \\, dx = \\frac{n}{\\theta ^{n}} \\frac{1}{n+1}\\theta ^{n+1} = \\frac{n\\theta}{n+1},\n",
    "$$\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\theta}^{(\\mathrm{MLE})}) = \\mathbb{E}[(\\hat{\\theta}^{(\\mathrm{MLE})})^{2}] - \\mathbb{E}[\\hat{\\theta}^{(\\mathrm{MLE})}]^{2} = \\frac{n\\theta^{2}}{n+2} - \\left( \\frac{n\\theta}{n+1} \\right)^{2} = \\frac{n\\theta^{2}}{(n+2)(n+1)^{2}}.\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MLE})},\\theta) = \\frac{2\\theta^{2}}{(n+2)(n+1)} \\le \\frac{\\theta^{2}}{3n} = \\operatorname{MSE}d_{1},\\theta ).\n",
    "$$\n",
    "\n",
    "Thus, we can say that $\\hat{\\theta}^{(\\mathrm{MLE})} = \\max_{i}X _i$ is a better estimator than $d_{1} = 2\\overline{X}$.\n",
    "\n",
    "We compare the MLE with previous estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_estimator(sample):\n",
    "    return np.max(sample, axis=0)\n",
    "\n",
    "# Plot MSE vs sample size n\n",
    "estimators_mse['MLE'] = lambda n: 2*theta_true**2 / ((n + 2) * (n + 1))\n",
    "plot_mse(plt.subplot(2, 1, 1), estimators_mse, n_mse)\n",
    "\n",
    "# Plot histograms of the estimators\n",
    "estimators['MLE'] = mle_estimator\n",
    "if estimates.shape[0] < len(estimators):\n",
    "    estimates = np.vstack((estimates, mle_estimator(samples_hist)))\n",
    "else:\n",
    "    id = list(estimators.keys()).index('MLE')\n",
    "    estimates[id, :] = mle_estimator(samples_hist)\n",
    "plot_hist(plt.subplot(2, 1, 2), estimators, estimates)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d281713",
   "metadata": {},
   "source": [
    "## Uniformly Minimum-Variance Unbiased Estimator\n",
    "\n",
    "Can we do better? Equation $(1)$ suggests that $\\frac{n+1}{n}\\hat{\\theta}^{(\\mathrm{MLE})}$ is an unbiased estimator, which may further reduce the error.\n",
    "\n",
    "We have the following fact:\n",
    "\n",
    "> [!prop] Proposition ^prop\n",
    ">\n",
    "> If $T$ is a complete and [[Sufficient Statistic]] for a parameter $\\theta$, and $\\phi(T)$ is an estimator dependent only on $T$, then $\\phi(T)$ is the unique minimum-variance unbiased estimator (UMVUE) of $\\mathbb{E}_{\\theta }\\phi(T)$.\n",
    "\n",
    "We note that $\\max_{i}X_{i}$ is a complete and sufficient statistic for $\\theta$. Thus, the estimator\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{UMVUE})} = \\frac{n+1}{n}\\max_{i}X_{i},\n",
    "$$\n",
    "has the minimum variance among all unbiased estimators of $\\theta$.\n",
    "\n",
    "We calculate its variance, i.e., MSE:\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{UMVUE})}) = \\operatorname{Var}\\left( \\frac{n+1}{n}\\hat{\\theta}^{(\\mathrm{MLE})} \\right) = \\left( \\frac{n+1}{n} \\right)^{2}\\operatorname{Var}(\\hat{\\theta}^{(\\mathrm{MLE})}) = \\frac{\\theta^{2}}{n(n+2)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def umvue_estimator(sample):\n",
    "    n = len(sample)\n",
    "    return (n+1) / n * np.max(sample, axis=0)\n",
    "\n",
    "# Plot MSE vs sample size n\n",
    "estimators_mse['UMVUE'] = lambda n: theta_true**2 / (n * (n + 2))\n",
    "plot_mse(plt.subplot(2, 1, 1), estimators_mse, n_mse)\n",
    "\n",
    "# Plot histograms of the estimators\n",
    "estimators['UMVUE'] = umvue_estimator\n",
    "if estimates.shape[0] < len(estimators):\n",
    "    estimates = np.vstack((estimates, umvue_estimator(samples_hist)))\n",
    "else:\n",
    "    id = list(estimators.keys()).index('UMVUE')\n",
    "    estimates[id, :] = umvue_estimator(samples_hist)\n",
    "plot_hist(plt.subplot(2, 1, 2), estimators, estimates)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b89499",
   "metadata": {},
   "source": [
    "## Jackknife\n",
    "\n",
    "In our problem, the bias of the MLE can be explicitly calculated, and thus we can directly correct it using the [[Best Estimator for Uniform Distribution Parameter#Uniformly Minimum-Variance Unbiased Estimator\\|UMVUE]]. For more general biased estimators, we can use Jackknife resampling to *estimate* the bias, and then correct the estimator.\n",
    "\n",
    "The first step of this procedure is to produce a series of [[Cross-Validation\\|leave-one-out]] estimates:\n",
    "$$\n",
    "\\hat{\\theta}_{(-i)} = \\hat{\\theta}_{\\{ X_{1},\\dots,X_{i-1},X_{i+1},\\dots,X_n \\}}.\n",
    "$$\n",
    "That is, we remove one sample $X_{i}$ and construct the estimator $\\hat{\\theta}_{(-i)}$ using the remaining samples.\n",
    "Then, the Jackknife bias estimate is\n",
    "$$\n",
    "\\widehat{\\operatorname{Bias}}(\\hat{\\theta}_{n}) = (n-1)\\left( \\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\theta}_{(-i)} - \\hat{\\theta}_{n} \\right) ,\n",
    "$$\n",
    "where $\\hat{\\theta}_{n}$ is the original estimator (with $n$ samples) to be corrected.\n",
    "Thus, the corrected Jackknife estimator is\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{Jack})} = \\hat{\\theta} - \\widehat{\\operatorname{Bias}}(\\hat{\\theta}_{n}) = n\\hat{\\theta}_{n} -  \\frac{n-1}{n}\\sum_{i=1}^{n}\\hat{\\theta}_{(-i)}.\n",
    "$$\n",
    "\n",
    "Generally, the MSE of a Jackknife estimator is difficult to calculate as $\\hat{\\theta}_{(-i)}$ are correlated. However, if we are to correct the MLE estimator, the Jackknife estimator has a simple form:\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{Jack})} = \\frac{2n-1}{n} X_{(n)} - \\frac{n-1}{n}X_{(n-1)},\n",
    "$$\n",
    "where $X_{(i)}$ is the $i$-th order statistic of the sample $X_{1},\\dots,X_{n}$, and thus $X_{(n)}=\\max_{i}X_{i}$.\n",
    "Moreover, we can calculate its MSE, which slightly improves that of the MLE estimator:\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{Jack})}) = \\left( 1- \\frac{n-1}{n^{2}} \\right) \\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MLE})}).\n",
    "$$\n",
    "\n",
    "See [[Best Estimator for Uniform Distribution Parameter#Appendix]] for details of the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jackknife_estimator(sample):\n",
    "    mle = mle_estimator(sample)\n",
    "    # Produce leave-one-out MLEs\n",
    "    mle_loo = 0\n",
    "    n = sample.shape[0]\n",
    "    for i in range(n):\n",
    "        mle_loo = mle_loo + mle_estimator(np.delete(sample, i, axis=0))\n",
    "    return n * mle - (n - 1) / n * mle_loo \n",
    "\n",
    "# Plot MSE vs sample size n\n",
    "estimators_mse['Jackknife'] = lambda n: 2 * (n**2 - n + 1) * theta_true**2 / (n**2 * (n + 1) * (n + 2))\n",
    "plot_mse(plt.subplot(2, 1, 1), estimators_mse, n_mse)\n",
    "\n",
    "# Plot histograms of the estimators\n",
    "estimators['Jackknife'] = jackknife_estimator\n",
    "if estimates.shape[0] < len(estimators):\n",
    "    estimates = np.vstack((estimates, jackknife_estimator(samples_hist)))\n",
    "else:\n",
    "    id = list(estimators.keys()).index('Jackknife')\n",
    "    estimates[id, :] = jackknife_estimator(samples_hist)\n",
    "plot_hist(plt.subplot(2, 1, 2), estimators, estimates)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e51cf",
   "metadata": {},
   "source": [
    "## Minimal MSE\n",
    "\n",
    "When we look at the MSEs, for [[Best Estimator for Uniform Distribution Parameter#Maximum Likelihood Estimation\\|MLE]], bias dominates, for [[Best Estimator for Uniform Distribution Parameter#Uniformly Minimum-Variance Unbiased Estimator\\|UMVUE]] and [[Best Estimator for Uniform Distribution Parameter#Jackknife]], variance dominates.\n",
    "A natural next step is to find the estimator that achieves the optimal balance between [[Bias-Variance Trade-Off\\|bias and variance]]. Actually, such an estimator is indeed the *best* estimator for $\\theta$ in terms of MSE (see [[Best Estimator for Uniform Distribution Parameter#Appendix]]).\n",
    "\n",
    "Consider a general form of the MLE and UMVUE using the complete and sufficient statistic $X _{(n)}$:\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{MMSE})} = cX_{(n)}\n",
    "$$\n",
    "Then, we have\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\theta}^{(\\mathrm{MMSE})}] = \\frac{cn\\theta}{n+1},\n",
    "\\quad\n",
    "\\operatorname{Var}(\\hat{\\theta}^{(\\mathrm{MMSE})}) = \\frac{c^{2}n\\theta^{2}}{(n+1)^{2}(n+2)}.\n",
    "$$\n",
    "Thus,\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MMSE})}) = \\frac{\\theta^{2}}{(n+1)^{2}(n+2)} (\\underbrace{c^{2}n + (n+1-cn)^{2}(n+2)}_{f(c)}) \n",
    "$$\n",
    "Setting\n",
    "$$\n",
    "\\frac{ \\mathrm{d}f }{ \\mathrm{d}c } = 2nc - 2n(n+2)(n+1-cn) = 0\n",
    "$$\n",
    "gives $c = \\frac{n+2}{n+1}$. Therefore, we get\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{MMSE})} = \\frac{n+2}{n+1} \\max_{i}X_{i},\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{MMSE})}) = \\frac{\\theta^{2}}{(n+1)^{2}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmse_estimator(sample):\n",
    "    n = sample.shape[0]\n",
    "    return (n + 2) / (n + 1) * np.max(sample, axis=0)\n",
    "\n",
    "# Plot MSE vs sample size n\n",
    "estimators_mse['MMSE'] = lambda n: theta_true**2 / ((n + 1) ** 2)\n",
    "plot_mse(plt.subplot(2, 1, 1), estimators_mse, n_mse)\n",
    "\n",
    "# Plot histograms of the estimators\n",
    "estimators['MMSE'] = mmse_estimator\n",
    "if estimates.shape[0] < len(estimators):\n",
    "    estimates = np.vstack((estimates, mmse_estimator(samples_hist)))\n",
    "else:\n",
    "    id = list(estimators.keys()).index('MMSE')\n",
    "    estimates[id, :] = mmse_estimator(samples_hist)\n",
    "plot_hist(plt.subplot(2, 1, 2), estimators, estimates)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5debc82a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The following table summarizes the estimators we have discussed.\n",
    "\n",
    "| Estimator | Expression | Bias | Variance | MSE |\n",
    "|-----------|------------|------|----------|-----|\n",
    "| $k$ Samples | $\\frac{2}{k} \\sum_{i=1}^{k} X_i$ | $0$ | $\\frac{\\theta^2}{3k}$ | $\\frac{\\theta^2}{3k}$ |\n",
    "| Method of Moments | $2\\overline{X}$ | $0$ | $\\frac{\\theta^2}{3n}$ | $\\frac{\\theta^2}{3n}$ |\n",
    "| MLE | $X_{(n)}$ | $-\\frac{\\theta}{n+1}$ | $\\frac{n\\theta^2}{(n+2)(n+1)^2}$ | $\\frac{2\\theta^2}{(n+2)(n+1)}$ |\n",
    "| UMVUE | $\\frac{n+1}{n} X_{(n)}$ | $0$ | $\\frac{\\theta^2}{n(n+2)}$ | $\\frac{\\theta^2}{n(n+2)}$ |\n",
    "| Jackknife | $\\frac{2n-1}{n} X_{(n)} - \\frac{n-1}{n} X_{(n-1)}$ | $-\\frac{\\theta}{n(n+1)}$ | $\\frac{(2n^2 - 1)\\theta^2}{n(n+1)^2(n+2)}$ | $\\frac{2(n^2 - n + 1)\\theta^2}{n^2(n+1)(n+2)}$ |\n",
    "| MMSE | $\\frac{n+2}{n+1} X_{(n)}$ | $-\\frac{\\theta}{(n+1)^{2}}$ | $\\frac{n(n+2)\\theta^2}{(n+1)^4}$ | $\\frac{\\theta^2}{(n+1)^2}$ |\n",
    "\n",
    "Finally, we plot the histograms of all estimators separately to compare their distributions, and calculate their empirical mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of all estimators separately\n",
    "num_bins = 20\n",
    "range_bins = (0.8, 1.2)\n",
    "y_lim_high = 0\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"]()\n",
    "\n",
    "fig, ax = plt.subplots(round(len(estimators) / 2), 2)\n",
    "for i, (name, estimator) in enumerate(estimators.items()):\n",
    "    plt.subplot(round(len(estimators) / 2), 2, i + 1)\n",
    "    # Match the color increment\n",
    "    sns.histplot(estimates[i, :], stat='density', bins=num_bins, binrange=range_bins, alpha=0.5, color=next(colors)['color'])\n",
    "    plt.axvline(theta_true, color='red', linestyle='--', label='$\\\\theta$')\n",
    "    plt.title(f'{name}')\n",
    "    # log the y lim\n",
    "    y_lim_high = max(y_lim_high, plt.ylim()[1])\n",
    "    \n",
    "plt.setp(ax, ylim=(0, y_lim_high))\n",
    "fig.supxlabel('$\\\\hat{\\\\theta}$')\n",
    "fig.supylabel('Density')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbaf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate empirical MSE for each estimator\n",
    "num_simulations = int(1e)\n",
    "mse_empirical = np.empty((len(estimators), len(sample_sizes), num_simulations))\n",
    "mse_empirical[:] = np.nan\n",
    "for n in sample_sizes:\n",
    "    samples = np.random.uniform(0, theta_true, size=(n, num_simulations))\n",
    "    estimates = np.empty((num_simulations, len(estimators)))\n",
    "    for i, (name, estimator) in enumerate(estimators.items()):\n",
    "        mse_empirical[i, sample_sizes.index(n), :] = (estimator(samples) - theta_true)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e22d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot empirical MSE with 0.95 confidence region using fill_between for each estimator in a single plot\n",
    "plt.figure()\n",
    "for i, (name, estimator) in enumerate(estimators.items()):\n",
    "    mean_mse = np.mean(mse_empirical[i, :, :], axis=1)\n",
    "    std_mse = np.std(mse_empirical[i, :, :], axis=1)\n",
    "    err_high = mean_mse + 1.96 * std_mse / np.sqrt(num_simulations)\n",
    "    err_low = mean_mse - 1.96 * std_mse / np.sqrt(num_simulations)\n",
    "\n",
    "    plt.plot(sample_sizes, mean_mse, label=name)\n",
    "    plt.fill_between(sample_sizes, err_low, err_high, alpha=0.2)\n",
    "    # Remove legend handles for the fill_between\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Sample Size n')\n",
    "plt.ylabel('Empirical MSE')\n",
    "plt.title('Empirical MSE of Estimators vs Sample Size')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0d7d6",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Calculation of Jackknife MSE\n",
    "\n",
    "For correcting the MLE $X_{(n)}$, the Jackknife estimator has a simple form:\n",
    "$$\n",
    "\\hat{\\theta}^{(\\mathrm{Jack})} = n X_{(n)} - \\frac{n-1}{n}\\left( (n-1)X_{(n)} + X_{(n-1)} \\right) \n",
    "= \\frac{2n-1}{n} X_{(n)} - \\frac{n-1}{n}X_{(n-1)},\n",
    "$$\n",
    "where $X_{(i)}$ is the $i$-th order statistic of the sample $X_{1},\\dots,X_{n}$, and thus $X_{(n)}=\\max_{i}X_{i}$.\n",
    "\n",
    "Similar to the calculation in [[Best Estimator for Uniform Distribution Parameter#Maximum Likelihood Estimation]] (see also [[Order Statistics#Distribution of i-th Order Statistic]]), the PDF of $X_{(n-1)}$ is\n",
    "$$\n",
    "f_{(n-1)}(x) = \\frac{n!}{(n-2)!} \\frac{(x /\\theta)^{n-2}(1-x /\\theta)}{\\theta} = \\frac{n(n-1)x^{n-2}(1-x /\\theta)}{\\theta ^{n-1}}.\n",
    "$$\n",
    "Then, we can calculate the bias:\n",
    "$$\n",
    "\\operatorname{Bias}(\\hat{\\theta}^{(\\mathrm{Jack})}) =\\frac{2n-1}{n} \\frac{n\\theta}{n+1} - \\frac{n-1}{n} \\frac{n(n-1)}{\\theta ^{n-1}}\\left( \\frac{\\theta ^{n}}{n} - \\frac{\\theta ^{n+1}}{(n+1)\\theta } \\right) -\\theta  = -\\frac{1}{n^{2}+n}\\theta .\n",
    "$$\n",
    "We can see that compared to the [[Best Estimator for Uniform Distribution Parameter#Maximum Likelihood Estimation]], the bias is significantly reduced.\n",
    "\n",
    "To calculate the variance of $\\hat{\\theta}^{(\\mathrm{Jack})}$, we need to know the joint distribution of $X_{(n)}$ and $X_{(n-1)}$. Note that the [[Order Statistics#Joint Distribution\\|joint PDF of the entire order statistic]] is given by\n",
    "$$\n",
    "f_{(X_{(i)})} (x) = \\frac{n!}{\\theta ^{n}} \\mathbb{1}\\{ x_{1}\\le \\dots\\le x_{n} \\}.\n",
    "$$\n",
    "Integrating out $(X_{(1)},\\dots ,X_{(n-2)})$ gives\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_{(X_{(n-1)},X_{(n)})}(x_{n-1},x_{n}) =&\\frac{n!}{\\theta ^{n}}\\int _{0}^{x_{n-1}}\\int _{0}^{x_{n-2}}\\cdots\\int _{0}^{x_{2}} \\d x_{1} \\cdots  \\, \\d x_{n-3} \\d x_{n-2} \\\\\n",
    "=&\\frac{n!}{\\theta ^{n}}\\int _{0}^{x_{n-1} }\\int _{0}^{x_{n-2}}\\frac{1}{(n-4)!}x_{n-3}^{n-4} \\d x_{n-3}  \\d x_{n-2} \\\\\n",
    "=&n(n-1) \\frac{x_{n-1}^{n-2}}{\\theta ^{n}}, \\quad 0 \\le x_{n-1} \\le x_{n} \\le \\theta.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Thus, their covariance is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Cov(X_{(n-1)},X_{(n)}) =& \\mathbb{E}[X_{(n-1)}X_{(n)}] - \\mathbb{E}[X_{(n-1)}]\\mathbb{E}[X_{(n)}]\\\\\n",
    "=&\\int_{0}^{\\theta}\\int_{0}^{y} x y f_{(X_{(n-1)},X_{(n)})}(x,y) \\mathrm{d}x \\mathrm{d}y - \\frac{n\\theta}{n+1}  \\frac{(n-1)\\theta}{n+1}\\\\\n",
    "=& \\frac{n(n-1)}{\\theta^{n}} \\frac{\\theta^{n+2}}{n(n+2)} - \\frac{n\\theta}{n+1}  \\frac{(n-1)\\theta}{n+1} \\\\\n",
    "=& \\frac{(n-1)\\theta^{2}}{(n+1)^{2}(n+2)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Then, we can calculate the variance:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Var(\\hat{\\theta}^{(\\mathrm{Jack})}) =& \\frac{(2n-1)^{2}}{n^{2}}\\Var(X_{(n)}) + \\frac{(n-1)^{2}}{n^{2}}\\Var(X_{(n-1)}) - 2 \\frac{(2n-1)(n-1)}{n^{2}}\\Cov(X_{(n)},X_{(n-1)}) \\\\\n",
    "=&  \\frac{(2n-1)^{2}}{n^{2}}\\frac{n\\theta^{2}}{(n+2)(n+1)^{2}} \n",
    "+ \\frac{(n-1)^{2}}{n^{2}}\\frac{2(n-1)\\theta^{2}}{(n+1)^{2}(n+2)} \n",
    "- 2 \\frac{(2n-1)(n-1)}{n^{2}}\\frac{(n-1)\\theta^{2}}{(n+1)^{2}(n+2)} \\\\\n",
    "=& \\frac{\\theta^{2}}{(n+2)(n+1)^{2}} \\left( \\frac{(2n-1)^{2}}{n} + \\frac{2(n-1)^{3}}{n^{2}} - \\frac{2(2n-1)(n-1)^{2}}{n^{2}}\\right) \\\\\n",
    "=& \\frac{(2n^{2}-1)\\theta^{2}}{n(n+1)^{2}(n+2)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally, we get\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta}^{(\\mathrm{Jack})}) = \\operatorname{Var}(\\hat{\\theta}^{(\\mathrm{Jack})}) + \\operatorname{Bias}(\\hat{\\theta}^{(\\mathrm{Jack})})^{2} = \\frac{2(n^{3}+1)\\theta^{2}}{n^{2}(n+1)^{2}(n+2)} = \\frac{2(n^{2}-n+1)}{n^{2}(n+1)(n+2)} \\theta^{2} .\n",
    "$$\n",
    "\n",
    "### Optimality of MMSE\n",
    "\n",
    "For any estimator $\\hat{\\theta}$, we write $\\mathbb{E}\\hat{\\theta}_{X} = f_n(\\theta)$. Suppose $f$ is linear, then by the linearity of expectation, we have\n",
    "$$\n",
    "\\mathbb{E}f_{n}\\left( \\frac{n+1}{n}X_{(n)} \\right) = f_{n} \\left( \\frac{n+1}{n} \\mathbb{E}X_{(n)} \\right)  = f_{n}(\\theta ) = \\mathbb{E}\\hat{\\theta}_{X}.\n",
    "$$\n",
    "Let $\\phi(T) = f_{n}\\left( \\frac{n+1}{n}T \\right)$. By Proposition [[Best Estimator for Uniform Distribution Parameter#^prop]], we know $\\phi(X_{(n)})$ has the same bias as but smaller variance than $\\hat{\\theta}_{X}$ if $\\hat{\\theta}_{X}\\ne \\phi(T)$ (uniqueness). Further, among the class of estimators consisting of linear functions of $X_{(n)}$, it is easy to see $\\hat{\\theta}^{(\\mathrm{MMSE})}$ has the smallest MSE.\n",
    "\n",
    "Now suppose $f_{n}$ is not linear. By Taylor expansion,\n",
    "$$\n",
    "f_{n}(\\theta) = \\sum_{k=0}^{\\infty} \\frac{f_n^{(k)}(0)}{k!} \\theta^k.\n",
    "$$\n",
    "For $\\hat{\\theta}$ to be uniformly optimal for any $\\theta$, it must satisties\n",
    "$$\n",
    "f_n(0) -\\theta  = o(\\theta) \\text{ as }\\theta \\to 0  \\quad\\text{and}\\quad f_{n}(\\theta) -\\theta  = O(\\theta) \\text{ as } \\theta\\to \\infty.\n",
    "$$\n",
    "Thus, $f_{n}^{(k)} = 0$ for $k\\ne 1$ and any $n$, indicating that $f_{n}$ must be linear in $\\theta$."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
